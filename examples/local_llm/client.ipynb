{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本地大型语言模型 (LLM)\n",
    "\n",
    "在这里，我们将使用一个为本地大型语言模型 (LLM) 提供服务的服务器。\n",
    "\n",
    "**注意**：这适用于原型设计/开发用途，但当可能存在来自不同用户的并发请求时，不应用于生产环境。截至目前，Ollama 被设计为单用户使用，无法处理并发请求，详见此问题：[Parallel requests · Issue #358 · ollama/ollama](https://github.com/ollama/ollama/issues/358)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain langchain_community langserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langserve import RemoteRunnable\n",
    "\n",
    "model = RemoteRunnable(\"http://localhost:8000/ollama/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们测试一下聊天模型的标准接口。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"请听我讲述一个关于猫咪的三句话故事。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='等你准备好了，按你的要求开始讲。', response_metadata={'model': 'llama3.1', 'created_at': '2024-09-04T05:35:46.345586Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 3021375389, 'load_duration': 34721363, 'prompt_eval_count': 25, 'prompt_eval_duration': 229295000, 'eval_count': 13, 'eval_duration': 2755150000}, id='run-1bbabe1a-d0fb-41dd-ab32-6556bab4d227-0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='当然可以！你先说第一句。', response_metadata={'model': 'llama3.1', 'created_at': '2024-09-04T05:35:50.165361Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 2218906968, 'load_duration': 101420104, 'prompt_eval_count': 25, 'prompt_eval_duration': 232897000, 'eval_count': 10, 'eval_duration': 1882589000}, id='run-b0623861-7700-4536-9be3-d0e4f73bf10b-0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await model.ainvoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量API是有效的，但由于ollama不支持并行处理，它的速度并不比调用.invoke两次快。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.16 ms, sys: 2.79 ms, total: 8.95 ms\n",
      "Wall time: 23.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='好吧，请开始讲故事了！', response_metadata={'model': 'llama3.1', 'created_at': '2024-09-04T05:52:52.23673Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 20209445573, 'load_duration': 11892913269, 'prompt_eval_count': 25, 'prompt_eval_duration': 3085068000, 'eval_count': 9, 'eval_duration': 5227245000}, id='run-b02192fd-645c-4c0e-8ac6-53120281861d-0'),\n",
       " AIMessage(content='当然可以！我很高兴地听你讲述这个关于猫咪的故事。', response_metadata={'model': 'llama3.1', 'created_at': '2024-09-04T05:52:55.135358Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 23108231846, 'load_duration': 11894639816, 'prompt_eval_count': 25, 'prompt_eval_duration': 3020765000, 'eval_count': 21, 'eval_duration': 5105026000}, id='run-9962638d-5bf7-4161-ae2a-f4db255fcee0-0')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.batch([prompt, prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.84 ms, sys: 2.03 ms, total: 8.87 ms\n",
      "Wall time: 8.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for _ in range(2):\n",
    "    model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='好的，请开始吧！', response_metadata={'model': 'llama3.1', 'created_at': '2024-09-04T05:53:14.715299Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 1934266873, 'load_duration': 31591576, 'prompt_eval_count': 25, 'prompt_eval_duration': 260860000, 'eval_count': 6, 'eval_duration': 1433818000}, id='run-7fc9f68a-a995-42cc-bf3a-8500b16ba361-0'),\n",
       " AIMessage(content='好的，请发表您的故事！', response_metadata={'model': 'llama3.1', 'created_at': '2024-09-04T05:53:14.931201Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 2150361879, 'load_duration': 30730413, 'prompt_eval_count': 25, 'prompt_eval_duration': 205887000, 'eval_count': 8, 'eval_duration': 1910737000}, id='run-596b6060-9719-418d-8948-acd7ef4fd17c-0')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await model.abatch([prompt, prompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "流式传输默认可用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好|啊|！|我|准备|好了|，你|可以|开始|讲|故事|了|！||"
     ]
    }
   ],
   "source": [
    "for chunk in model.stream(prompt):\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当然|，可以|听|你|讲|述|这个|故事|了|。||"
     ]
    }
   ],
   "source": [
    "async for chunk in model.astream(prompt):\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事件流API同样可用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chat_model_start', 'run_id': '7ab179d9-f924-4fa4-8561-c242307b5e32', 'name': '/ollama', 'tags': [], 'metadata': {}, 'data': {'input': '请听我讲述一个关于猫咪的三句话故事。'}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '7ab179d9-f924-4fa4-8561-c242307b5e32', 'tags': [], 'metadata': {}, 'name': '/ollama', 'data': {'chunk': AIMessageChunk(content='好', id='run-7ab179d9-f924-4fa4-8561-c242307b5e32')}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '7ab179d9-f924-4fa4-8561-c242307b5e32', 'tags': [], 'metadata': {}, 'name': '/ollama', 'data': {'chunk': AIMessageChunk(content='啊', id='run-7ab179d9-f924-4fa4-8561-c242307b5e32')}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '7ab179d9-f924-4fa4-8561-c242307b5e32', 'tags': [], 'metadata': {}, 'name': '/ollama', 'data': {'chunk': AIMessageChunk(content='，我', id='run-7ab179d9-f924-4fa4-8561-c242307b5e32')}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '7ab179d9-f924-4fa4-8561-c242307b5e32', 'tags': [], 'metadata': {}, 'name': '/ollama', 'data': {'chunk': AIMessageChunk(content='很', id='run-7ab179d9-f924-4fa4-8561-c242307b5e32')}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '7ab179d9-f924-4fa4-8561-c242307b5e32', 'tags': [], 'metadata': {}, 'name': '/ollama', 'data': {'chunk': AIMessageChunk(content='想', id='run-7ab179d9-f924-4fa4-8561-c242307b5e32')}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '7ab179d9-f924-4fa4-8561-c242307b5e32', 'tags': [], 'metadata': {}, 'name': '/ollama', 'data': {'chunk': AIMessageChunk(content='听', id='run-7ab179d9-f924-4fa4-8561-c242307b5e32')}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '7ab179d9-f924-4fa4-8561-c242307b5e32', 'tags': [], 'metadata': {}, 'name': '/ollama', 'data': {'chunk': AIMessageChunk(content='！', id='run-7ab179d9-f924-4fa4-8561-c242307b5e32')}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '7ab179d9-f924-4fa4-8561-c242307b5e32', 'tags': [], 'metadata': {}, 'name': '/ollama', 'data': {'chunk': AIMessageChunk(content='', response_metadata={'model': 'llama3.1', 'created_at': '2024-09-04T05:55:03.819327Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 1805311385, 'load_duration': 29989536, 'prompt_eval_count': 25, 'prompt_eval_duration': 234317000, 'eval_count': 8, 'eval_duration': 1539253000}, id='run-7ab179d9-f924-4fa4-8561-c242307b5e32')}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_end', 'name': '/ollama', 'run_id': '7ab179d9-f924-4fa4-8561-c242307b5e32', 'tags': [], 'metadata': {}, 'data': {'output': AIMessageChunk(content='好啊，我很想听！', response_metadata={'model': 'llama3.1', 'created_at': '2024-09-04T05:55:03.819327Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 1805311385, 'load_duration': 29989536, 'prompt_eval_count': 25, 'prompt_eval_duration': 234317000, 'eval_count': 8, 'eval_duration': 1539253000}, id='run-7ab179d9-f924-4fa4-8561-c242307b5e32')}, 'parent_ids': []}\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "async for event in model.astream_events(prompt, version='v1'):\n",
    "    print(event)\n",
    "    if i > 10:\n",
    "        print('...')\n",
    "        break\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
